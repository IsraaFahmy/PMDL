{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment1_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNdarz1uIIZ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/KhaledElTahan/DeepLearning/blob/master/Labs/lab1/lab1_part1.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSA7aTOivRhA"
      },
      "source": [
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfATj8pltlxh"
      },
      "source": [
        "# Lab 1 - Part1: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOy1mYdKw5TE"
      },
      "source": [
        "![Linear Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/linear_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLKJ-4kwLYv"
      },
      "source": [
        "## 1.1.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxlb182u_gv"
      },
      "source": [
        "The problem we are trying to solve here is finding a new house which is suitable to our needs and the budget we assigned. The client who wants to buy the new house did her research and found some houses. She wrote the details of each house she visited including location, sale condition, sale type, house price, among others. She needs some help to know how much she is expected to pay to get a house that conforms with her specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUidwfyPvcfv"
      },
      "source": [
        "Your task is to build a linear regression model that helps her to predict the house price depending on the given attributes she collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXilc2Nvg-g"
      },
      "source": [
        "## 1.1.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHAbLgL1MsY"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n5JS__dwfiH"
      },
      "source": [
        "### 1.1.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm9MPDVSsDpu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfciecZwm6L"
      },
      "source": [
        "### 1.1.2.2 Configure Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW1TyciwsN9_"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PJSAiyx4HM"
      },
      "source": [
        "### 1.1.2.3 Work on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVsD6V1x-Ty"
      },
      "source": [
        "This dataset contains 80 features that demonstrate the state of the house and our target which is the house price.\n",
        "\n",
        "We begin by loading the train and test splits of the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6zYNeXVvvRA"
      },
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyl1lDq-yVFG"
      },
      "source": [
        "You can have a look at the train split of the dataset using the head command. I very much encourage you to have a deeper look on the dataset file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2JWObzgyY9t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "2d5e3c5f-c566-47bc-86cb-42d2371bd986"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>...</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
              "0   1          60       RL  ...        WD         Normal    208500\n",
              "1   2          20       RL  ...        WD         Normal    181500\n",
              "2   3          60       RL  ...        WD         Normal    223500\n",
              "3   4          70       RL  ...        WD        Abnorml    140000\n",
              "4   5          60       RL  ...        WD         Normal    250000\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXVoqAU0h1g"
      },
      "source": [
        "Data preprocessing:\n",
        "* First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
        "* Create Dummy variables for the categorical features\n",
        "* Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCCi1svksOj4"
      },
      "source": [
        "# Concatenate all the data\n",
        "# We do this to be able to preprocess on the whole dataset\n",
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Log transform the target y in training data - by reference inside all\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "# Log transform skewed numeric features:\n",
        "\n",
        "# Get Numerical Fields\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index \n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns\n",
        "skewed_feats = skewed_feats.index # Get Skewed Columns indices\n",
        "\n",
        "# Log scale skewed columns\n",
        "# Normalize the skewed distribution for better regression\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
        "\n",
        "# Create Dummy variables for the categorical features \n",
        "all_data = pd.get_dummies(all_data) \n",
        "\n",
        "# Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the data to training & testing\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "#split training data into training & validation, default splitting is 25% validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVa9K1Ga1vlo"
      },
      "source": [
        "### 1.1.2.4 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IocosMrh2W3X"
      },
      "source": [
        "One important note you need to be aware of, linear regression is a neural network with only one perceptron (i.e. dense layer with one node) with a linear activation (i.e. no activation function). \n",
        "\n",
        "![One Perceptron Neural Network](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron.png)\n",
        "\n",
        "Use this note to define a **sequential model of one dense layer with one unit using Tensorflow.Keras**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1NvkNXsVVG"
      },
      "source": [
        "    from tensorflow.keras.layers import Dropout\n",
        "    from tensorflow.keras.layers import BatchNormalization\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=X_tr.shape[1], activation=None))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSa178n-2BM-"
      },
      "source": [
        "### 1.1.2.5 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6Yy5oDsXyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf8ef9b-af29-4291-9f52-17a8eb6205c8"
      },
      "source": [
        "model.compile(loss = \"mean_squared_error\", optimizer = \"Adam\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_65 (Dense)             (None, 1)                 289       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 293\n",
            "Trainable params: 291\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgD-Kqo3fwb"
      },
      "source": [
        "### 1.1.2.6 Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4pLcN73waK"
      },
      "source": [
        "Fit your model into the training data, use the validation data to be able to plot the loss decrement during the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkW1KTm3TGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a127129f-7af0-442b-fab6-97959e4938ef"
      },
      "source": [
        "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "35/35 [==============================] - 1s 5ms/step - loss: 145.3042 - val_loss: 144.1842\n",
            "Epoch 2/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 143.9352 - val_loss: 143.2553\n",
            "Epoch 3/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 143.4230 - val_loss: 142.2160\n",
            "Epoch 4/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 142.9206 - val_loss: 141.0825\n",
            "Epoch 5/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 141.8348 - val_loss: 140.2503\n",
            "Epoch 6/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 140.7031 - val_loss: 139.5734\n",
            "Epoch 7/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 139.5390 - val_loss: 138.5309\n",
            "Epoch 8/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 139.1224 - val_loss: 137.8869\n",
            "Epoch 9/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 138.6836 - val_loss: 136.9645\n",
            "Epoch 10/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 137.4020 - val_loss: 136.2587\n",
            "Epoch 11/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 136.4318 - val_loss: 135.5488\n",
            "Epoch 12/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 135.8762 - val_loss: 134.5725\n",
            "Epoch 13/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 134.8060 - val_loss: 133.8526\n",
            "Epoch 14/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 133.7850 - val_loss: 133.1794\n",
            "Epoch 15/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 132.6297 - val_loss: 132.3292\n",
            "Epoch 16/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 132.3188 - val_loss: 131.4514\n",
            "Epoch 17/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 131.3979 - val_loss: 130.7281\n",
            "Epoch 18/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 130.8853 - val_loss: 129.7761\n",
            "Epoch 19/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 129.7979 - val_loss: 129.0558\n",
            "Epoch 20/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 129.1515 - val_loss: 128.2377\n",
            "Epoch 21/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 129.3095 - val_loss: 127.5471\n",
            "Epoch 22/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 127.3585 - val_loss: 126.8156\n",
            "Epoch 23/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 126.6827 - val_loss: 126.0280\n",
            "Epoch 24/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 126.3560 - val_loss: 125.2001\n",
            "Epoch 25/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 125.2100 - val_loss: 124.4242\n",
            "Epoch 26/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 124.6273 - val_loss: 123.7084\n",
            "Epoch 27/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 124.0168 - val_loss: 122.9669\n",
            "Epoch 28/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 122.7230 - val_loss: 122.1447\n",
            "Epoch 29/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 122.3116 - val_loss: 121.3593\n",
            "Epoch 30/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 121.1981 - val_loss: 120.6096\n",
            "Epoch 31/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 120.8774 - val_loss: 119.8826\n",
            "Epoch 32/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 119.9230 - val_loss: 119.1657\n",
            "Epoch 33/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 119.1589 - val_loss: 118.3497\n",
            "Epoch 34/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 118.4608 - val_loss: 117.6299\n",
            "Epoch 35/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 117.9746 - val_loss: 116.8599\n",
            "Epoch 36/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 117.1106 - val_loss: 116.1831\n",
            "Epoch 37/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 116.6156 - val_loss: 115.5244\n",
            "Epoch 38/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 115.7105 - val_loss: 114.7938\n",
            "Epoch 39/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 114.8060 - val_loss: 114.1100\n",
            "Epoch 40/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 114.2802 - val_loss: 113.3464\n",
            "Epoch 41/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 113.6190 - val_loss: 112.6790\n",
            "Epoch 42/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 112.9661 - val_loss: 111.9557\n",
            "Epoch 43/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 111.8352 - val_loss: 111.2499\n",
            "Epoch 44/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 111.6287 - val_loss: 110.5811\n",
            "Epoch 45/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 110.6645 - val_loss: 109.8932\n",
            "Epoch 46/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 109.9289 - val_loss: 109.2109\n",
            "Epoch 47/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 109.3596 - val_loss: 108.5374\n",
            "Epoch 48/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 108.5642 - val_loss: 107.8518\n",
            "Epoch 49/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 107.7519 - val_loss: 107.2232\n",
            "Epoch 50/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 107.5546 - val_loss: 106.4909\n",
            "Epoch 51/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 106.2691 - val_loss: 105.8110\n",
            "Epoch 52/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 105.9143 - val_loss: 105.1250\n",
            "Epoch 53/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 104.8643 - val_loss: 104.4338\n",
            "Epoch 54/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 104.3132 - val_loss: 103.7284\n",
            "Epoch 55/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 103.8633 - val_loss: 103.0800\n",
            "Epoch 56/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 103.5329 - val_loss: 102.4092\n",
            "Epoch 57/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 102.5015 - val_loss: 101.7128\n",
            "Epoch 58/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 101.7032 - val_loss: 101.0660\n",
            "Epoch 59/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 100.9784 - val_loss: 100.4469\n",
            "Epoch 60/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 100.3680 - val_loss: 99.8442\n",
            "Epoch 61/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 100.1805 - val_loss: 99.2458\n",
            "Epoch 62/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 99.0989 - val_loss: 98.5311\n",
            "Epoch 63/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 98.4264 - val_loss: 97.8357\n",
            "Epoch 64/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 98.0726 - val_loss: 97.1976\n",
            "Epoch 65/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 96.8649 - val_loss: 96.5824\n",
            "Epoch 66/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 96.7896 - val_loss: 95.9976\n",
            "Epoch 67/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 96.2275 - val_loss: 95.3498\n",
            "Epoch 68/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 95.7738 - val_loss: 94.6451\n",
            "Epoch 69/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 94.9310 - val_loss: 93.9967\n",
            "Epoch 70/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 94.1384 - val_loss: 93.3873\n",
            "Epoch 71/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 93.2584 - val_loss: 92.7226\n",
            "Epoch 72/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 92.7022 - val_loss: 92.1195\n",
            "Epoch 73/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 92.2748 - val_loss: 91.5078\n",
            "Epoch 74/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 91.6257 - val_loss: 90.8857\n",
            "Epoch 75/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 90.9220 - val_loss: 90.2705\n",
            "Epoch 76/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 90.0857 - val_loss: 89.6461\n",
            "Epoch 77/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 89.5317 - val_loss: 89.0139\n",
            "Epoch 78/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 89.2098 - val_loss: 88.4724\n",
            "Epoch 79/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 88.7395 - val_loss: 87.9193\n",
            "Epoch 80/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 87.9670 - val_loss: 87.2734\n",
            "Epoch 81/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 87.2879 - val_loss: 86.6402\n",
            "Epoch 82/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 87.0745 - val_loss: 86.1110\n",
            "Epoch 83/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 86.0950 - val_loss: 85.5170\n",
            "Epoch 84/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 85.4772 - val_loss: 84.8793\n",
            "Epoch 85/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 85.1113 - val_loss: 84.2060\n",
            "Epoch 86/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 84.3935 - val_loss: 83.6239\n",
            "Epoch 87/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 83.7009 - val_loss: 82.9585\n",
            "Epoch 88/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 83.2611 - val_loss: 82.3912\n",
            "Epoch 89/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 82.6075 - val_loss: 81.9008\n",
            "Epoch 90/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 82.6637 - val_loss: 81.3529\n",
            "Epoch 91/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 81.3838 - val_loss: 80.7974\n",
            "Epoch 92/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 80.7504 - val_loss: 80.1566\n",
            "Epoch 93/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 80.2149 - val_loss: 79.6166\n",
            "Epoch 94/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 79.5920 - val_loss: 79.0161\n",
            "Epoch 95/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 78.9328 - val_loss: 78.5128\n",
            "Epoch 96/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 78.2575 - val_loss: 77.8984\n",
            "Epoch 97/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 78.2080 - val_loss: 77.3143\n",
            "Epoch 98/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 77.6108 - val_loss: 76.7373\n",
            "Epoch 99/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 77.1290 - val_loss: 76.1713\n",
            "Epoch 100/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 76.4704 - val_loss: 75.6277\n",
            "Epoch 101/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 75.8425 - val_loss: 75.0832\n",
            "Epoch 102/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 75.0338 - val_loss: 74.5200\n",
            "Epoch 103/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.6026 - val_loss: 73.9803\n",
            "Epoch 104/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.2166 - val_loss: 73.3842\n",
            "Epoch 105/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 73.3277 - val_loss: 72.8047\n",
            "Epoch 106/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 73.0450 - val_loss: 72.2797\n",
            "Epoch 107/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 72.6191 - val_loss: 71.7300\n",
            "Epoch 108/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 72.0410 - val_loss: 71.2252\n",
            "Epoch 109/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 71.4719 - val_loss: 70.7222\n",
            "Epoch 110/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 70.7179 - val_loss: 70.1963\n",
            "Epoch 111/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 70.2406 - val_loss: 69.6675\n",
            "Epoch 112/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 69.4100 - val_loss: 69.1464\n",
            "Epoch 113/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 69.0156 - val_loss: 68.6134\n",
            "Epoch 114/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 68.6019 - val_loss: 68.0681\n",
            "Epoch 115/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 68.2237 - val_loss: 67.5245\n",
            "Epoch 116/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 67.6478 - val_loss: 67.0133\n",
            "Epoch 117/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 66.9430 - val_loss: 66.5861\n",
            "Epoch 118/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 66.6736 - val_loss: 66.0633\n",
            "Epoch 119/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.4369 - val_loss: 65.4795\n",
            "Epoch 120/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 65.7381 - val_loss: 64.9592\n",
            "Epoch 121/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 65.1769 - val_loss: 64.3830\n",
            "Epoch 122/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 64.5006 - val_loss: 63.9178\n",
            "Epoch 123/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 63.9391 - val_loss: 63.4258\n",
            "Epoch 124/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 63.7286 - val_loss: 62.9462\n",
            "Epoch 125/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 63.0383 - val_loss: 62.4681\n",
            "Epoch 126/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 62.6409 - val_loss: 61.9670\n",
            "Epoch 127/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 61.9888 - val_loss: 61.4570\n",
            "Epoch 128/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 61.6023 - val_loss: 60.9627\n",
            "Epoch 129/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 60.9450 - val_loss: 60.5083\n",
            "Epoch 130/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 60.2832 - val_loss: 60.0325\n",
            "Epoch 131/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 60.4715 - val_loss: 59.5531\n",
            "Epoch 132/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 59.7601 - val_loss: 59.0564\n",
            "Epoch 133/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 59.1638 - val_loss: 58.5809\n",
            "Epoch 134/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 58.6825 - val_loss: 58.0801\n",
            "Epoch 135/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 57.9722 - val_loss: 57.6200\n",
            "Epoch 136/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 57.7563 - val_loss: 57.1342\n",
            "Epoch 137/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 56.9741 - val_loss: 56.7038\n",
            "Epoch 138/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 56.8236 - val_loss: 56.1833\n",
            "Epoch 139/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 56.3407 - val_loss: 55.7532\n",
            "Epoch 140/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 55.7926 - val_loss: 55.2794\n",
            "Epoch 141/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 55.3199 - val_loss: 54.8666\n",
            "Epoch 142/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 54.9846 - val_loss: 54.3829\n",
            "Epoch 143/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 54.2092 - val_loss: 53.9455\n",
            "Epoch 144/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 53.8667 - val_loss: 53.4465\n",
            "Epoch 145/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 53.4361 - val_loss: 53.0038\n",
            "Epoch 146/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 53.1028 - val_loss: 52.5473\n",
            "Epoch 147/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.6872 - val_loss: 52.0459\n",
            "Epoch 148/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 52.2612 - val_loss: 51.6184\n",
            "Epoch 149/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 51.8532 - val_loss: 51.2062\n",
            "Epoch 150/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 51.4037 - val_loss: 50.8086\n",
            "Epoch 151/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 50.7828 - val_loss: 50.3819\n",
            "Epoch 152/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 50.0529 - val_loss: 49.9440\n",
            "Epoch 153/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 50.1550 - val_loss: 49.4889\n",
            "Epoch 154/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.6359 - val_loss: 49.0428\n",
            "Epoch 155/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 48.9918 - val_loss: 48.6523\n",
            "Epoch 156/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 48.5737 - val_loss: 48.2074\n",
            "Epoch 157/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 48.2918 - val_loss: 47.7494\n",
            "Epoch 158/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 47.5803 - val_loss: 47.3458\n",
            "Epoch 159/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 47.6923 - val_loss: 46.9313\n",
            "Epoch 160/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 47.0566 - val_loss: 46.4459\n",
            "Epoch 161/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 46.7145 - val_loss: 46.0108\n",
            "Epoch 162/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 46.0501 - val_loss: 45.5842\n",
            "Epoch 163/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 45.6109 - val_loss: 45.1760\n",
            "Epoch 164/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 45.3406 - val_loss: 44.7605\n",
            "Epoch 165/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 44.9163 - val_loss: 44.3162\n",
            "Epoch 166/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 44.3227 - val_loss: 43.9198\n",
            "Epoch 167/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 44.1493 - val_loss: 43.5343\n",
            "Epoch 168/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 43.7127 - val_loss: 43.1477\n",
            "Epoch 169/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 43.5073 - val_loss: 42.7550\n",
            "Epoch 170/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 42.8832 - val_loss: 42.3940\n",
            "Epoch 171/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.5481 - val_loss: 42.0216\n",
            "Epoch 172/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 42.1318 - val_loss: 41.6179\n",
            "Epoch 173/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 41.6394 - val_loss: 41.2085\n",
            "Epoch 174/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 41.1569 - val_loss: 40.7567\n",
            "Epoch 175/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 41.0038 - val_loss: 40.3375\n",
            "Epoch 176/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 40.4049 - val_loss: 39.9594\n",
            "Epoch 177/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 40.1053 - val_loss: 39.6198\n",
            "Epoch 178/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 39.8286 - val_loss: 39.2050\n",
            "Epoch 179/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 39.3103 - val_loss: 38.8029\n",
            "Epoch 180/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 38.9952 - val_loss: 38.4002\n",
            "Epoch 181/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 38.5122 - val_loss: 38.0526\n",
            "Epoch 182/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 38.0504 - val_loss: 37.6670\n",
            "Epoch 183/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 37.6200 - val_loss: 37.3161\n",
            "Epoch 184/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 37.6085 - val_loss: 36.9559\n",
            "Epoch 185/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 36.9524 - val_loss: 36.6033\n",
            "Epoch 186/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 36.6798 - val_loss: 36.2193\n",
            "Epoch 187/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 36.3127 - val_loss: 35.8273\n",
            "Epoch 188/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 35.8462 - val_loss: 35.4712\n",
            "Epoch 189/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 35.6578 - val_loss: 35.0897\n",
            "Epoch 190/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 35.1747 - val_loss: 34.7454\n",
            "Epoch 191/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.7604 - val_loss: 34.3462\n",
            "Epoch 192/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.3760 - val_loss: 34.0082\n",
            "Epoch 193/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.2795 - val_loss: 33.6246\n",
            "Epoch 194/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 33.8147 - val_loss: 33.2847\n",
            "Epoch 195/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 33.4560 - val_loss: 32.9620\n",
            "Epoch 196/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.0643 - val_loss: 32.6660\n",
            "Epoch 197/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 32.6202 - val_loss: 32.3210\n",
            "Epoch 198/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 32.2864 - val_loss: 32.0278\n",
            "Epoch 199/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 32.1447 - val_loss: 31.6935\n",
            "Epoch 200/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 31.8718 - val_loss: 31.3227\n",
            "Epoch 201/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 31.5023 - val_loss: 31.0026\n",
            "Epoch 202/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.9763 - val_loss: 30.6563\n",
            "Epoch 203/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.6750 - val_loss: 30.3165\n",
            "Epoch 204/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.5128 - val_loss: 29.9576\n",
            "Epoch 205/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.1107 - val_loss: 29.6378\n",
            "Epoch 206/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 29.7797 - val_loss: 29.3012\n",
            "Epoch 207/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 29.1886 - val_loss: 28.9907\n",
            "Epoch 208/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 29.0752 - val_loss: 28.7052\n",
            "Epoch 209/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.9739 - val_loss: 28.4029\n",
            "Epoch 210/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.4352 - val_loss: 28.0913\n",
            "Epoch 211/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.1709 - val_loss: 27.7686\n",
            "Epoch 212/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 27.9028 - val_loss: 27.4431\n",
            "Epoch 213/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.3252 - val_loss: 27.1451\n",
            "Epoch 214/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 27.1903 - val_loss: 26.8287\n",
            "Epoch 215/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 27.0784 - val_loss: 26.5319\n",
            "Epoch 216/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.6211 - val_loss: 26.2241\n",
            "Epoch 217/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.3518 - val_loss: 25.9546\n",
            "Epoch 218/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.9154 - val_loss: 25.6346\n",
            "Epoch 219/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.5515 - val_loss: 25.3356\n",
            "Epoch 220/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.1837 - val_loss: 25.0589\n",
            "Epoch 221/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.0492 - val_loss: 24.7569\n",
            "Epoch 222/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 24.9558 - val_loss: 24.5030\n",
            "Epoch 223/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.5898 - val_loss: 24.1872\n",
            "Epoch 224/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 24.2156 - val_loss: 23.9345\n",
            "Epoch 225/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.9391 - val_loss: 23.6168\n",
            "Epoch 226/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.6641 - val_loss: 23.3223\n",
            "Epoch 227/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.3574 - val_loss: 23.0439\n",
            "Epoch 228/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.9500 - val_loss: 22.7676\n",
            "Epoch 229/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.7935 - val_loss: 22.4576\n",
            "Epoch 230/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.6549 - val_loss: 22.2032\n",
            "Epoch 231/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.3799 - val_loss: 21.9077\n",
            "Epoch 232/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.1448 - val_loss: 21.6609\n",
            "Epoch 233/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.5859 - val_loss: 21.4145\n",
            "Epoch 234/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.4190 - val_loss: 21.1416\n",
            "Epoch 235/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 21.0493 - val_loss: 20.9033\n",
            "Epoch 236/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.9412 - val_loss: 20.6168\n",
            "Epoch 237/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.5549 - val_loss: 20.3308\n",
            "Epoch 238/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.1909 - val_loss: 20.0804\n",
            "Epoch 239/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.1577 - val_loss: 19.7866\n",
            "Epoch 240/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.9635 - val_loss: 19.5249\n",
            "Epoch 241/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 19.3926 - val_loss: 19.2847\n",
            "Epoch 242/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 19.2580 - val_loss: 19.0241\n",
            "Epoch 243/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 19.2145 - val_loss: 18.7731\n",
            "Epoch 244/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 18.8087 - val_loss: 18.5069\n",
            "Epoch 245/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 18.5553 - val_loss: 18.2721\n",
            "Epoch 246/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 18.3351 - val_loss: 18.0378\n",
            "Epoch 247/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 18.1477 - val_loss: 17.7662\n",
            "Epoch 248/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 17.8590 - val_loss: 17.5210\n",
            "Epoch 249/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.4967 - val_loss: 17.2623\n",
            "Epoch 250/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 17.3668 - val_loss: 17.0314\n",
            "Epoch 251/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 17.0182 - val_loss: 16.7997\n",
            "Epoch 252/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 16.9303 - val_loss: 16.5686\n",
            "Epoch 253/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 16.6248 - val_loss: 16.3453\n",
            "Epoch 254/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 16.3881 - val_loss: 16.0875\n",
            "Epoch 255/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 16.1587 - val_loss: 15.8529\n",
            "Epoch 256/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.7536 - val_loss: 15.6198\n",
            "Epoch 257/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.7108 - val_loss: 15.4099\n",
            "Epoch 258/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.3532 - val_loss: 15.2117\n",
            "Epoch 259/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.0853 - val_loss: 15.0126\n",
            "Epoch 260/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.9194 - val_loss: 14.8064\n",
            "Epoch 261/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.9995 - val_loss: 14.5641\n",
            "Epoch 262/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.5862 - val_loss: 14.3531\n",
            "Epoch 263/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.3529 - val_loss: 14.1379\n",
            "Epoch 264/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.1197 - val_loss: 13.9169\n",
            "Epoch 265/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.9981 - val_loss: 13.6860\n",
            "Epoch 266/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.8351 - val_loss: 13.4946\n",
            "Epoch 267/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.5714 - val_loss: 13.2865\n",
            "Epoch 268/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.4085 - val_loss: 13.1051\n",
            "Epoch 269/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.0490 - val_loss: 12.8623\n",
            "Epoch 270/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.9574 - val_loss: 12.6630\n",
            "Epoch 271/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.8506 - val_loss: 12.5053\n",
            "Epoch 272/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.5060 - val_loss: 12.3006\n",
            "Epoch 273/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.2394 - val_loss: 12.1066\n",
            "Epoch 274/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.1720 - val_loss: 11.9094\n",
            "Epoch 275/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.9555 - val_loss: 11.7525\n",
            "Epoch 276/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.7912 - val_loss: 11.5688\n",
            "Epoch 277/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.4697 - val_loss: 11.3754\n",
            "Epoch 278/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.4630 - val_loss: 11.1717\n",
            "Epoch 279/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.2502 - val_loss: 10.9544\n",
            "Epoch 280/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.1172 - val_loss: 10.7488\n",
            "Epoch 281/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 10.7782 - val_loss: 10.5806\n",
            "Epoch 282/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.6264 - val_loss: 10.4199\n",
            "Epoch 283/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.4655 - val_loss: 10.2306\n",
            "Epoch 284/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.2432 - val_loss: 10.0832\n",
            "Epoch 285/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.1888 - val_loss: 9.9095\n",
            "Epoch 286/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.8577 - val_loss: 9.7574\n",
            "Epoch 287/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.6801 - val_loss: 9.5818\n",
            "Epoch 288/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.6246 - val_loss: 9.4331\n",
            "Epoch 289/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.3833 - val_loss: 9.2701\n",
            "Epoch 290/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.2619 - val_loss: 9.0975\n",
            "Epoch 291/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.2702 - val_loss: 8.9377\n",
            "Epoch 292/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.0124 - val_loss: 8.7664\n",
            "Epoch 293/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 8.7843 - val_loss: 8.6057\n",
            "Epoch 294/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.7053 - val_loss: 8.4442\n",
            "Epoch 295/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.4493 - val_loss: 8.2950\n",
            "Epoch 296/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 8.4134 - val_loss: 8.1341\n",
            "Epoch 297/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.1916 - val_loss: 7.9817\n",
            "Epoch 298/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.9282 - val_loss: 7.8391\n",
            "Epoch 299/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.9718 - val_loss: 7.6834\n",
            "Epoch 300/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.6710 - val_loss: 7.5288\n",
            "Epoch 301/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.5193 - val_loss: 7.3987\n",
            "Epoch 302/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.4268 - val_loss: 7.2743\n",
            "Epoch 303/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.2717 - val_loss: 7.1148\n",
            "Epoch 304/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 7.2432 - val_loss: 6.9749\n",
            "Epoch 305/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.9817 - val_loss: 6.8260\n",
            "Epoch 306/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.8292 - val_loss: 6.7053\n",
            "Epoch 307/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.8444 - val_loss: 6.5606\n",
            "Epoch 308/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.6784 - val_loss: 6.4288\n",
            "Epoch 309/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.5513 - val_loss: 6.2961\n",
            "Epoch 310/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.3790 - val_loss: 6.1777\n",
            "Epoch 311/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.1595 - val_loss: 6.0347\n",
            "Epoch 312/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.0815 - val_loss: 5.9070\n",
            "Epoch 313/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 6.0818 - val_loss: 5.7871\n",
            "Epoch 314/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.8899 - val_loss: 5.6700\n",
            "Epoch 315/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.7481 - val_loss: 5.5632\n",
            "Epoch 316/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.6766 - val_loss: 5.4498\n",
            "Epoch 317/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.5612 - val_loss: 5.3147\n",
            "Epoch 318/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.3874 - val_loss: 5.1804\n",
            "Epoch 319/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.2533 - val_loss: 5.0709\n",
            "Epoch 320/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.0957 - val_loss: 4.9665\n",
            "Epoch 321/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.0417 - val_loss: 4.8565\n",
            "Epoch 322/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.9007 - val_loss: 4.7323\n",
            "Epoch 323/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.7885 - val_loss: 4.6292\n",
            "Epoch 324/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.6782 - val_loss: 4.5278\n",
            "Epoch 325/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.6334 - val_loss: 4.4209\n",
            "Epoch 326/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4860 - val_loss: 4.3185\n",
            "Epoch 327/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 4.4140 - val_loss: 4.2089\n",
            "Epoch 328/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.3456 - val_loss: 4.1122\n",
            "Epoch 329/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.1889 - val_loss: 4.0140\n",
            "Epoch 330/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 4.0136 - val_loss: 3.9375\n",
            "Epoch 331/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.8923 - val_loss: 3.8318\n",
            "Epoch 332/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.8597 - val_loss: 3.7457\n",
            "Epoch 333/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.8437 - val_loss: 3.6532\n",
            "Epoch 334/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.6692 - val_loss: 3.5597\n",
            "Epoch 335/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.6131 - val_loss: 3.4515\n",
            "Epoch 336/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.5976 - val_loss: 3.3597\n",
            "Epoch 337/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.3722 - val_loss: 3.2721\n",
            "Epoch 338/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.3253 - val_loss: 3.1986\n",
            "Epoch 339/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.2774 - val_loss: 3.1097\n",
            "Epoch 340/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.1372 - val_loss: 3.0120\n",
            "Epoch 341/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.0727 - val_loss: 2.9393\n",
            "Epoch 342/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.0378 - val_loss: 2.8671\n",
            "Epoch 343/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.9423 - val_loss: 2.7949\n",
            "Epoch 344/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.8616 - val_loss: 2.7233\n",
            "Epoch 345/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.7990 - val_loss: 2.6470\n",
            "Epoch 346/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.6558 - val_loss: 2.5793\n",
            "Epoch 347/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.6498 - val_loss: 2.4942\n",
            "Epoch 348/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.5938 - val_loss: 2.4123\n",
            "Epoch 349/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.5362 - val_loss: 2.3396\n",
            "Epoch 350/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.4567 - val_loss: 2.2739\n",
            "Epoch 351/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.3491 - val_loss: 2.2116\n",
            "Epoch 352/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.2594 - val_loss: 2.1446\n",
            "Epoch 353/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.2342 - val_loss: 2.0886\n",
            "Epoch 354/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1153 - val_loss: 2.0254\n",
            "Epoch 355/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.0380 - val_loss: 1.9649\n",
            "Epoch 356/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.0368 - val_loss: 1.9140\n",
            "Epoch 357/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.9251 - val_loss: 1.8526\n",
            "Epoch 358/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.8647 - val_loss: 1.7941\n",
            "Epoch 359/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.8697 - val_loss: 1.7359\n",
            "Epoch 360/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.7537 - val_loss: 1.6814\n",
            "Epoch 361/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8229 - val_loss: 1.6203\n",
            "Epoch 362/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.6632 - val_loss: 1.5700\n",
            "Epoch 363/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6483 - val_loss: 1.5163\n",
            "Epoch 364/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.5798 - val_loss: 1.4652\n",
            "Epoch 365/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5326 - val_loss: 1.4260\n",
            "Epoch 366/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.4675 - val_loss: 1.3712\n",
            "Epoch 367/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4274 - val_loss: 1.3211\n",
            "Epoch 368/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.3632 - val_loss: 1.2728\n",
            "Epoch 369/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.3207 - val_loss: 1.2163\n",
            "Epoch 370/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.3257 - val_loss: 1.1722\n",
            "Epoch 371/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2311 - val_loss: 1.1396\n",
            "Epoch 372/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.2075 - val_loss: 1.0964\n",
            "Epoch 373/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.1613 - val_loss: 1.0626\n",
            "Epoch 374/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 1.0241\n",
            "Epoch 375/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.0929 - val_loss: 0.9873\n",
            "Epoch 376/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.0658 - val_loss: 0.9494\n",
            "Epoch 377/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.0260 - val_loss: 0.9088\n",
            "Epoch 378/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.9853 - val_loss: 0.8759\n",
            "Epoch 379/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.8999 - val_loss: 0.8413\n",
            "Epoch 380/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.9443 - val_loss: 0.8126\n",
            "Epoch 381/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.8772 - val_loss: 0.7848\n",
            "Epoch 382/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.8406 - val_loss: 0.7489\n",
            "Epoch 383/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.8035 - val_loss: 0.7175\n",
            "Epoch 384/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.7423 - val_loss: 0.6848\n",
            "Epoch 385/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6506\n",
            "Epoch 386/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6894 - val_loss: 0.6221\n",
            "Epoch 387/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6806 - val_loss: 0.6017\n",
            "Epoch 388/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6495 - val_loss: 0.5797\n",
            "Epoch 389/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.6385 - val_loss: 0.5532\n",
            "Epoch 390/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.5355\n",
            "Epoch 391/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.5095\n",
            "Epoch 392/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5467 - val_loss: 0.4837\n",
            "Epoch 393/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5076 - val_loss: 0.4628\n",
            "Epoch 394/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4993 - val_loss: 0.4430\n",
            "Epoch 395/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5023 - val_loss: 0.4221\n",
            "Epoch 396/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4814 - val_loss: 0.4019\n",
            "Epoch 397/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4404 - val_loss: 0.3823\n",
            "Epoch 398/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4356 - val_loss: 0.3663\n",
            "Epoch 399/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.3520\n",
            "Epoch 400/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3924 - val_loss: 0.3365\n",
            "Epoch 401/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3902 - val_loss: 0.3179\n",
            "Epoch 402/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3945 - val_loss: 0.3021\n",
            "Epoch 403/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.2867\n",
            "Epoch 404/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 0.2751\n",
            "Epoch 405/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3491 - val_loss: 0.2633\n",
            "Epoch 406/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.3026 - val_loss: 0.2532\n",
            "Epoch 407/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2991 - val_loss: 0.2398\n",
            "Epoch 408/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2943 - val_loss: 0.2268\n",
            "Epoch 409/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2714 - val_loss: 0.2144\n",
            "Epoch 410/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2723 - val_loss: 0.2034\n",
            "Epoch 411/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 0.1948\n",
            "Epoch 412/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2525 - val_loss: 0.1829\n",
            "Epoch 413/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2378 - val_loss: 0.1759\n",
            "Epoch 414/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2219 - val_loss: 0.1660\n",
            "Epoch 415/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1600\n",
            "Epoch 416/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2166 - val_loss: 0.1533\n",
            "Epoch 417/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.1460\n",
            "Epoch 418/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1886 - val_loss: 0.1383\n",
            "Epoch 419/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1827 - val_loss: 0.1315\n",
            "Epoch 420/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1635 - val_loss: 0.1249\n",
            "Epoch 421/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 0.1187\n",
            "Epoch 422/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1561 - val_loss: 0.1141\n",
            "Epoch 423/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 0.1095\n",
            "Epoch 424/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1686 - val_loss: 0.1023\n",
            "Epoch 425/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1683 - val_loss: 0.0992\n",
            "Epoch 426/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 0.0946\n",
            "Epoch 427/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.0918\n",
            "Epoch 428/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 0.0880\n",
            "Epoch 429/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.0857\n",
            "Epoch 430/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1281 - val_loss: 0.0817\n",
            "Epoch 431/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.0800\n",
            "Epoch 432/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.0768\n",
            "Epoch 433/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.0747\n",
            "Epoch 434/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1142 - val_loss: 0.0726\n",
            "Epoch 435/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.0690\n",
            "Epoch 436/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 0.0679\n",
            "Epoch 437/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0663\n",
            "Epoch 438/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.0657\n",
            "Epoch 439/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.0628\n",
            "Epoch 440/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.0620\n",
            "Epoch 441/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.0601\n",
            "Epoch 442/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1094 - val_loss: 0.0597\n",
            "Epoch 443/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 0.0586\n",
            "Epoch 444/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0970 - val_loss: 0.0576\n",
            "Epoch 445/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1036 - val_loss: 0.0568\n",
            "Epoch 446/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.0555\n",
            "Epoch 447/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.0541\n",
            "Epoch 448/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 0.0548\n",
            "Epoch 449/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0952 - val_loss: 0.0531\n",
            "Epoch 450/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1012 - val_loss: 0.0531\n",
            "Epoch 451/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.0530\n",
            "Epoch 452/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.0537\n",
            "Epoch 453/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0535\n",
            "Epoch 454/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.0532\n",
            "Epoch 455/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.0530\n",
            "Epoch 456/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.0520\n",
            "Epoch 457/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.0489\n",
            "Epoch 458/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.0498\n",
            "Epoch 459/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.0502\n",
            "Epoch 460/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0814 - val_loss: 0.0509\n",
            "Epoch 461/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.0504\n",
            "Epoch 462/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.0504\n",
            "Epoch 463/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0983 - val_loss: 0.0504\n",
            "Epoch 464/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.0505\n",
            "Epoch 465/500\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0981 - val_loss: 0.0497\n",
            "Epoch 466/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.0497\n",
            "Epoch 467/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.0493\n",
            "Epoch 468/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.0494\n",
            "Epoch 469/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.0494\n",
            "Epoch 470/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0871 - val_loss: 0.0491\n",
            "Epoch 471/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.0498\n",
            "Epoch 472/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.0498\n",
            "Epoch 473/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.0501\n",
            "Epoch 474/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.0505\n",
            "Epoch 475/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0921 - val_loss: 0.0499\n",
            "Epoch 476/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 0.0488\n",
            "Epoch 477/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.0498\n",
            "Epoch 478/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.0497\n",
            "Epoch 479/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0932 - val_loss: 0.0495\n",
            "Epoch 480/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.0491\n",
            "Epoch 481/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.0499\n",
            "Epoch 482/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.0506\n",
            "Epoch 483/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.0502\n",
            "Epoch 484/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.0500\n",
            "Epoch 485/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.0503\n",
            "Epoch 486/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0509\n",
            "Epoch 487/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0768 - val_loss: 0.0506\n",
            "Epoch 488/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0867 - val_loss: 0.0510\n",
            "Epoch 489/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0853 - val_loss: 0.0513\n",
            "Epoch 490/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 0.0505\n",
            "Epoch 491/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0906 - val_loss: 0.0508\n",
            "Epoch 492/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.0522\n",
            "Epoch 493/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.0513\n",
            "Epoch 494/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1148 - val_loss: 0.0508\n",
            "Epoch 495/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0898 - val_loss: 0.0505\n",
            "Epoch 496/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0935 - val_loss: 0.0508\n",
            "Epoch 497/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0862 - val_loss: 0.0516\n",
            "Epoch 498/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0964 - val_loss: 0.0501\n",
            "Epoch 499/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1006 - val_loss: 0.0497\n",
            "Epoch 500/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.0510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdUbgUd4Ifw"
      },
      "source": [
        "And this is how you can predict an output for any number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaO-wCLl3Wd_",
        "outputId": "31c814fd-2407-4945-fe59-68404477352d"
      },
      "source": [
        "print(model.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[72.206436]\n",
            " [66.763824]\n",
            " [70.076294]\n",
            " ...\n",
            " [70.375374]\n",
            " [63.566864]\n",
            " [72.23912 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VhmPDN4Sva"
      },
      "source": [
        "### 1.1.2.7 Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bDCQffM4Zh2"
      },
      "source": [
        "It's time to see how your model's progress during the training, If all is good, you will find the validation loss decreasing without neither overfitting nor underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEZ7SdwI2e_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "047bcc9e-5a03-4e67-afc2-9859acc117f7"
      },
      "source": [
        "\n",
        "# Get training and test loss histories\n",
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 388,
              "height": 261
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gni9AWDueGU"
      },
      "source": [
        "## 1.1.3 Conclusion:\n",
        "\n",
        "That's it! Congratulations on training a linear regression model. \n",
        "\n",
        "Make sure you finish the second part of the assignment and deliver all the requirements for the submission.\n",
        "\n"
      ]
    }
  ]
}